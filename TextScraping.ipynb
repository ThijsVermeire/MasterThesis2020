{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Scraping\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and inputs\n",
    "### 1.1 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import api as api\n",
    "import numpy as np\n",
    "import requests\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "from stocknews import StockNews\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import uuid\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basic functions\n",
    " - Read names of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_tickers():\n",
    "    tickers = []\n",
    "    try:\n",
    "        connection = get_connection()\n",
    "        cursor = connection.cursor()\n",
    "        select_query = \"\"\"select ticker from companies\"\"\"\n",
    "        cursor.execute(select_query,)\n",
    "        records = cursor.fetchall()\n",
    "        for row in records:\n",
    "            tickers.append(row[0])\n",
    "        close_connection(connection)\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error while getting data\", error)\n",
    "    return tickers\n",
    "\n",
    "def get_connection():\n",
    "    connection = psycopg2.connect(user = \"postgres\",\n",
    "                                  password = \"postgres\",\n",
    "                                  host = \"localhost\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    return connection\n",
    "\n",
    "def close_connection(connection):\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        print(\"Postgres connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Scrape tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 Load Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oauth_keys = [\n",
    "    [\"dfc6Wf3dTENuTlXDV1f9hml6B\", \"50vjNuiW6V5rZSRWvCyZy9RSY4h134Y9CjHHUtaxKbzqCdnn6D\", \"1359361622-LwyudniQqFogVrRXwoerQCSugRmU95nYKpWPrfS\", \"4nSO0bMINyisgrRfZgOD4SbAUOVcgC7BRq1N9j2AdTq5U\"],\n",
    "    [\"lZkQXF2uZgljsr84A9ZnToFrS\", \"1fSeS1NRnSJW5rLV3snOg2NJOAVSzymKBumEbS40Lo4cge1Hwu\", \"1299631912659623936-ti2P7XgEfZkBdoOwlveFILCTOgShKG\", \"COffmtis6f3hbTyxbKOrNDfOUAygjoubpex4ytaXXsJzo\"],\n",
    "    [\"lUwGGzNTUOiO16pzmxCLFrgCj\", \"FLH2utmpJ9y0HLQDcdlKKeGl4ZheY6s7osa3QzUGSYJGL1TkWt\", \"1299630171419533313-Knb2DgpfxEhENpqKkGvaTGXDwOFMhQ\", \"q0UvNWfxccp9AuCjGuxpBbBKbvSsywZ6Cbsk3hVDzhE1X\"]\n",
    "    ]\n",
    "\n",
    "auths = []\n",
    "for consumer_key, consumer_secret, access_key, access_secret in oauth_keys:\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    auths.append(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Write to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_tweets(df):\n",
    "    try:\n",
    "        connection = get_connection()\n",
    "        cursor = connection.cursor()\n",
    "        postgres_insert_query = \"\"\" INSERT INTO tweets (ticker , text, createdat , tweetid , coordinates, userid , userfollowers) VALUES (%s,%s,%s,%s,%s,%s,%s)\"\"\"\n",
    "        for i in range(len(df.Ticker)):\n",
    "            record_to_insert = (df['Ticker'][i], df['tweet.text'][i], df['tweet.created_at'][i], str(df['tweet.id_str'][i]), str(df['tweet.coordinates'][i]), str(df['tweet.user.id'][i]), str(df['tweet.user.followers_count'][i]))\n",
    "            cursor.execute(postgres_insert_query,record_to_insert)\n",
    "            connection.commit()\n",
    "        close_connection(connection)\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error while getting data \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Scrape tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_tweets_main():\n",
    "#     for ticker in get_tickers()[::-1]:\n",
    "    for ticker in get_tickers():\n",
    "        auth = auths[0]\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "        max_tweets = 200\n",
    "\n",
    "        # Creation of query method using parameters\n",
    "        yesteday = datetime.date.today() + datetime.timedelta(-1)\n",
    "        last_closing_time = datetime.datetime(yesteday.year, yesteday.month,yesteday.day, 22,0,0)\n",
    "        text_query = str(ticker) + ' -filter:retweets'\n",
    "        tweets = tweepy.Cursor(api.search,q=ticker, until = last_closing_time, tweet_mode = 'extended', lang = 'en').items(max_tweets)\n",
    "\n",
    "        # Pulling information from tweets iterable object\n",
    "        # Add or remove tweet information in the below list comprehension\n",
    "        tweets_list = [[tweet.full_text, tweet.created_at, tweet.id_str, tweet.coordinates, tweet.user.id, tweet.user.followers_count] for tweet in tweets]\n",
    "\n",
    "         #Check if tweet_list is not empty otherwise scrape next ticker\n",
    "        if(len(tweets_list) != 0):\n",
    "            # Creation of dataframe from tweets_list\n",
    "            tweets_df = pd.DataFrame(tweets_list)\n",
    "            tweets_df.columns = ['tweet.text', 'tweet.created_at', 'tweet.id_str','tweet.coordinates', 'tweet.user.id', 'tweet.user.followers_count']\n",
    "            tweets_df['Ticker'] = ticker\n",
    "            write_tweets(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scrape Yahoo finance stock news\n",
    " - Use package stocknews,\n",
    " which extracts news according given symbols and calculates the average sentiment of the summary and title\n",
    "\n",
    "### 3.1 Scrape Yahoo finance with package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_news_main():\n",
    "    stocks = get_tickers()\n",
    "    sn = StockNews(stocks, wt_key='MY_WORLD_TRADING_DATA_KEY')\n",
    "    tuple = sn.summarize()\n",
    "    df =pd.DataFrame(tuple)[0][0]\n",
    "    write_news(df= df)\n",
    "\n",
    "\n",
    "\n",
    "def write_news(df):\n",
    "    try:\n",
    "        connection = get_connection()\n",
    "        cursor = connection.cursor()\n",
    "        postgres_insert_query = \"\"\" INSERT INTO news (id, stock , news_dt, check_day , sentiment_summary_avg , sentiment_title_avg, sentiment_summary_med , sentiment_title_med) VALUES (%s,%s,%s,%s,%s,%s,%s,%s)\"\"\"\n",
    "        for i in range(len(df.id)):\n",
    "            record_to_insert = (df['id'][i], df['stock'][i], df['news_dt'][i], df['check_day'][i], df['sentiment_summary_avg'][i], df['sentiment_title_avg'][i], df['sentiment_summary_med'][i], df['sentiment_title_med'][i])\n",
    "            cursor.execute(postgres_insert_query,record_to_insert)\n",
    "            connection.commit()\n",
    "        close_connection(connection)\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error while getting data \", error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Scrape YF with BS4\n",
    "#### 3.2.1 Page 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_yf_news(df):\n",
    "    try:\n",
    "        connection = get_connection()\n",
    "        cursor = connection.cursor()\n",
    "        postgres_insert_query = \"\"\" INSERT INTO yf_news (id, title , text, date ) VALUES (%s,%s,%s,%s)\"\"\"\n",
    "        for i, val in enumerate(df.check_day):\n",
    "            id = uuid.uuid1()\n",
    "            record_to_insert = ( id.int , str(df[\"title\"][i]), str(df[\"text\"][i]), datetime.datetime.today())\n",
    "            cursor.execute(postgres_insert_query,record_to_insert)\n",
    "            connection.commit()\n",
    "        close_connection(connection)\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error while getting data \", error)\n",
    "\n",
    "\n",
    "\n",
    "def scrape_yf():\n",
    "\n",
    "    url = 'https://finance.yahoo.com/news/'\n",
    "    r = requests.get(url)\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    table = soup.find('div', {\"id\": \"Fin-Stream\"})\n",
    "    rows = table.find_all('li')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        title = row.find_all('a')\n",
    "        title =  [ele.text.strip() for ele in title]\n",
    "        cols = row.find_all('p')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([title, [ele for ele in cols if ele]])\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['id'] = id = uuid.uuid1()\n",
    "    df['check_day'] = datetime.datetime.today().day\n",
    "    df.rename(columns={0: 'title', 1: 'text'}, inplace = True)\n",
    "    write_yf_news(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.1.2 Page 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_yf2():\n",
    "    url = 'https://news.yahoo.com/'\n",
    "    r = requests.get(url)\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    table = soup.find('ul', {\"class\": \"My(0) Ov(h) P(0) Wow(bw)\"})\n",
    "    rows = table.find_all('li')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        title = row.find_all('a')\n",
    "        title =  [ele.text.strip() for ele in title]\n",
    "        cols = row.find_all('p')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([title, [ele for ele in cols if ele]])\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['id'] = id = uuid.uuid1()\n",
    "    df['check_day'] = datetime.datetime.today().day\n",
    "    df.rename(columns={0: 'title', 1: 'text'}, inplace = True)\n",
    "    write_yf_news(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN DAILY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape tweets\n",
      "Postgres connection is closed\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340809010971328513, AAPL) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340809010971328513, MSFT) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340809010971328513, AMZN) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340102095584677888, BABA) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340784127084273664, FB) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340809180190486535, V) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340809010971328513, WMT) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340809010971328513, TSLA) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340106514586066945, PG) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340792975043936256, MA) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1339886667725881344, NVDA) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340809010971328513, HD) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1339624991948726272, DIS) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340809010971328513, NFLX) already exists.\n",
      "\n",
      "Error while getting data  duplicate key value violates unique constraint \"tweets_pk\"\n",
      "DETAIL:  Key (tweetid, ticker)=(1340809010971328513, INTC) already exists.\n",
      "\n",
      "\n",
      " Scrape Yahoo Finance stock news\n",
      "Postgres connection is closed\n",
      "Error while getting data  duplicate key value violates unique constraint \"news_pk\"\n",
      "DETAIL:  Key (id)=(AAPL_2020-11-11) already exists.\n",
      "\n",
      "Postgres connection is closed\n",
      "Postgres connection is closed\n",
      "\n",
      " Done for today (2020-12-23 08:08:10.802505)\n"
     ]
    }
   ],
   "source": [
    "print('Scrape tweets')\n",
    "scrape_tweets_main()\n",
    "\n",
    "print('\\n Scrape Yahoo Finance stock news')\n",
    "scrape_news_main()\n",
    "\n",
    "scrape_yf()\n",
    "scrape_yf2()\n",
    "\n",
    "print('\\n Done for today ({})'.format(datetime.datetime.today()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
